# Ethics & Responsible Use Statement

SemantIQ-M-Benchmarks is a scientific instrument for evaluating specific capabilities of AI models. It is **NOT** a measure of general intelligence, consciousness, or moral worth.

## 1. Limitations
- **Narrow Scope**: Our benchmarks measure adherence to specific semantic instructions. High scores do not imply that a model is "safe" for all use cases.
- **Bias**: While we strive for neutrality, all benchmarks contain inherent biases from their creators.
- **Anthropomorphism**: We discourage using terms like "understanding," "thinking," or "feeling" when describing model performance.

## 2. Prohibited Uses
We explicitly condemn the use of this benchmark for:
- **Human Ranking**: Evaluating humans or employee performance.
- **Automated Decision Making**: Using these scores as the *sole* criteria for deploying models in high-risk domains (medical, legal) without further domain-specific validation.

## 3. Recommended Uses
- **Model Debugging**: Identifying where a model fails to follow instructions.
- **Comparative Research**: Tracking progress of the field over time.
- **Safety Filtering**: Using the safety subsets to test model guardrails.

## 4. Reporting Ethical Concerns
If you believe a specific prompt or test case promotes hate speech, discrimination, or harm, please report it immediately via our Security/Ethics contact channel. We prioritize removing harmful content over preserving benchmark stability.
