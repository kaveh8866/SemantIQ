# Adoption Metrics (Non-Performance)

We measure the success of SemantIQ by its **utility to the community**, not by how high models score.

## Key Performance Indicators (KPIs)

### 1. Research Impact
- **Citations**: Number of academic papers citing `SemantIQ-M-Benchmarks`.
- **Method Usage**: Number of papers using SMF or HACS protocols (even on different data).
- **Replications**: Independent studies verifying our baseline results.

### 2. Community Engagement
- **Contributors**: Unique GitHub contributors (code + data).
- **Discussions**: Active threads on methodology.
- **Forks**: Number of active forks extending the framework.

### 3. Ecosystem Reach
- **Integrations**: Number of third-party tools (leaderboards, training frameworks) integrating SemantIQ.
- **Downloads**: PyPI/Docker pulls (proxy for usage).

## Prohibited Metrics (Anti-Patterns)
We do **NOT** optimize for:
- **"High Scores"**: We do not celebrate when models score 100%. In fact, that indicates benchmark failure (saturation).
- **Leaderboard Rank**: We do not track "who is winning".
- **Hype Mentions**: Social media volume is not a proxy for scientific value.
