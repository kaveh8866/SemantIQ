# Launch Communication Kit

## 1. Announcement Snippet (Social Media / Discord)
> **ðŸš€ Announcing SemantIQ-M-Benchmarks v0.1.0**
> 
> We are excited to release SemantIQ, an open-source framework for reproducible multimodal AI evaluation.
> 
> ðŸ”¬ **Features**:
> - **SMF**: Test abstract reasoning & bias.
> - **HACS**: Measure alignment with human cognition.
> - **Vision**: Evaluate T2I consistency.
> 
> ðŸ”— **GitHub**: https://github.com/kaveh8866/SemantIQ
> ðŸ“„ **Docs**: [Link to Pages]
> 
> #AI #Benchmarks #OpenScience #Reproducibility

## 2. Blog Post Outline (Draft)
**Title**: Beyond Leaderboards: Introducing SemantIQ-M-Benchmarks

- **The Problem**: Current benchmarks are often contaminated, closed-source, or focus purely on rote knowledge (MMLU).
- **The Solution**: A modular, locally-run framework that focuses on *cognitive patterns* (Reasoning, Symmetry, Vision).
- **Deep Dive**:
    - Explain SMF (Semantic Maturity).
    - Explain HACS (Human-AI Symmetry).
- **Call to Action**: Try it out, contribute a new category, run it on your local LLMs.

## 3. FAQ

**Q: Is this a leaderboard?**
A: No. We provide the tools for you to evaluate models in your own context. We do not maintain a central ranking.

**Q: Can I use this for commercial models?**
A: Yes, the code is Apache 2.0. The data is CC-BY 4.0.

**Q: How do I add a new benchmark?**
A: See `CONTRIBUTING.md`. We have a governance process for adding new modules.

## 4. "How to Cite" Snippet
(Include the BibTeX from `CITATION.cff`)
