# Public Messaging & Framing

## Core Identity
**SemantIQ-M-Benchmarks** is a scientific instrument for evaluating the cognitive and semantic capabilities of AI models. It is **not** a leaderboard for marketing hype.

### What It Is
- **A Research Tool**: For dissecting model behavior across specific cognitive dimensions (Reasoning, Vision, Self-Correction).
- **A Diagnostic Framework**: To find *why* a model fails, not just *that* it fails.
- **Transparent**: Open source, open data, open methodology.

### What It Is NOT
- **An IQ Test**: "Intelligence" is a metaphor; we measure specific task performance.
- **A Leaderboard**: We do not rank models into a single "best" list. Context matters.
- **A User Ranking**: We do not evaluate human intelligence or worth.

## Target Audiences

### 1. AI Researchers
*Message*: "Get granular insights into your model's failure modes. Reproducible, local, and modular."

### 2. Model Developers
*Message*: "Integrate robust regression testing into your training pipeline. Catch bias and hallucination early."

### 3. Educators & Students
*Message*: "Learn how LLM evaluation works under the hood. Explore the difference between generation and understanding."

## Responsible Use Guidance
- **Do not cherry-pick**: Report full results across all categories, not just the ones where your model scores high.
- **Cite versions**: "Model X scored 80%" is meaningless. "Model X scored 0.80 on SMF v1.0" is scientific.
- **Human-in-the-loop**: Use HACS protocols to validate automated metrics. Automated scoring is a proxy, not truth.
